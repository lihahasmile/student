{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 布尔查询之BSBI与索引压缩\n",
    "\n",
    "**李娅琦  2213603**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码框架\n",
    "\n",
    "本次实验具体包含的内容有：\n",
    "1. [索引构建 (40%)](#索引构建与检索-(40%)) 使用BSBI方法模拟在内存不足的情况下的索引构建方式，并应用于布尔查询\n",
    "2. [索引压缩 (30%)](#索引压缩-(30%)) 使用可变长编码对构建的索引进行压缩\n",
    "3. [布尔检索 (10%)](#布尔联合检索-(10%)) 对空格分隔的单词查询进行联合（与）布尔检索\n",
    "3. [实验报告 (10%)](#Report-(25%)) 描述你的代码并回答一些问题\n",
    "4. [额外的编码方式 (10%)](#额外的编码方式-(10%)) 鼓励使用额外的编码方式对索引进行压缩 (例如, gamma-encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引的构建与检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 映射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IdMap类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该类主要是实现`term-termID`以及`doc-docID`的转换，其中补充编写了函数`_get_str()`和`_get_id()`。\n",
    "\n",
    "函数`_get_str()`将id转换成string，首先判断是否有id存在，如果存在则返回对应的string；\n",
    "\n",
    "函数`_get_id()`将string转换为id，如果string已经有则直接返回string对应位置存放的id，否则将其加入到list中在返回对应的id值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_str(self, i):\n",
    "    ### Begin your code\n",
    "    if i < 0 or i >= len(self.id_to_str):\n",
    "        raise IndexError(\"termID out of range.\")\n",
    "    return self.id_to_str[i]\n",
    "    ### End your code\n",
    "def _get_id(self, s):\n",
    "    ### Begin your code\n",
    "    if s in self.str_to_id:\n",
    "      return self.str_to_id[s]\n",
    "    else:\n",
    "        new_id = len(self.id_to_str)\n",
    "        self.str_to_id[s] = new_id\n",
    "        self.id_to_str.append(s)\n",
    "        return new_id\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BSBIIndex类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在该类中主要实现分块处理创建索引，其中补充编写了函数`parse_block()`和`invert_write()`。\n",
    "\n",
    "函数`parse_block()`接受子目录路径作为参数，创建`td_pairs[]`用于存储索引并返回。主要思路如下：\n",
    "\n",
    "1. 遍历每个子目录（即每个block）中的每个文件，使用`doc_id_map()`函数得到docID。\n",
    "2. 每个文件按行遍历并通过分词得到token（即term），按照token使用`term_id_map()`函数得到termID。\n",
    "3. 将所得的`termID-docID`对存储至td_pairs[]中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_block(self, block_dir_relative):##参数是子目录路径\n",
    "    ### Begin your code\n",
    "    td_pairs = []  ## 存储termID-docID对 \n",
    "    block_dir_path = os.path.join(self.data_dir, block_dir_relative) ## block路径\n",
    "    # 遍历block中的文件\n",
    "    for file_name in os.listdir(block_dir_path):\n",
    "        doc_id = self.doc_id_map[os.path.join(block_dir_relative, file_name)]\n",
    "        with open(os.path.join(block_dir_path, file_name), 'r') as file:\n",
    "            for line in file.readlines():\n",
    "                tokens = line.split(' ')\n",
    "                for token in tokens:\n",
    "                    term_id = self.term_id_map[token.strip()]\n",
    "                    td_pairs.append((term_id, doc_id))\n",
    "    return td_pairs\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 倒排表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现InvertedIndexWriter类。和列表类似，该类提供了append()函数，但是倒排表不会存储在内存中而是直接写入到磁盘里。下面按照函数append()中给出的三个function来补充实现该函数：\n",
    "\n",
    "1. 使用`postings_encoding`对postings_list进行编码，变成字节数组的形式。\n",
    "2. 对于每个term构建`三元组（起始位置，文档数量，字节长度）`，并将三元组信息加入postings_dict中。\n",
    "3. 使用write()函数将编码的字节流写入磁盘索引文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append(self, term, postings_list):\n",
    "   ### Begin your code\n",
    "   postings = self.postings_encoding.encode(sorted(postings_list)) ##编码\n",
    "   if len(self.terms)!=0 :\n",
    "      last_term = self.terms[-1] \n",
    "   else:\n",
    "      last_term = None\n",
    "   if last_term is not None:\n",
    "      start_position = self.index_file.tell()  ##索引文件中的起始位置\n",
    "   else:\n",
    "      start_position = 0\n",
    "   number_of_postings = len(postings_list)  ##文档数量\n",
    "   length_in_bytes=len(postings)   ##字节长度 \n",
    "   self.postings_dict[term] = (start_position, number_of_postings, length_in_bytes)\n",
    "   self.terms.append(term)   \n",
    "   self.index_file.write(postings)   ##将字节流附加到磁盘上的索引文件中\n",
    "   ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现invert_write类，将解析得到的td_pairs转换为倒排表并写入磁盘。其中`invert_write()`函数的实现思路如下：\n",
    "\n",
    "1. 对于得到的td_pairs使用`lamda()函数`按照term进行排序。\n",
    "2. 遍历排序好的td_pairs将termID相同的docID进行合并存放在postings_dict[term_id]中。\n",
    "3. 将已经转换好的倒排表利用append()函数写入磁盘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_write(self, td_pairs, index):\n",
    "    ### Begin your code\n",
    "    td_pairs = sorted(td_pairs, key=lambda x: self.term_id_map[x[0]])\n",
    "    postings_dict = {}  ##td_pairs转换为倒排表\n",
    "    for term_id, doc_id in td_pairs:\n",
    "        if term_id not in postings_dict:\n",
    "            postings_dict[term_id] = []\n",
    "        postings_dict[term_id].append(doc_id)\n",
    "    for term_id, doc_ids in postings_dict.items():  ##写入磁盘\n",
    "        index.append(term_id, list(set(doc_ids)))\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "通过构建`InvertedIndex`的子类`InvertedIndexIterator`迭代地从磁盘上每次读取文件的一个倒排列表。其中补充编写的`_next_()`函数思路如下：\n",
    "\n",
    "1. 设置变量`current_position`和`index_file`分别为在文件中的位置以及对应的索引文件。\n",
    "2. 通过当前在文件中的位置得到`termID`并利用三元组信息得到termID对应的`posting_list`并返回（即termID以及对应的文档编号组）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _initialization_hook(self):\n",
    "    ### Begin your code\n",
    "    self.current_position=0 ##开始时从文件头\n",
    "    self.index_file = open(self.index_file_path,\"rb+\")\n",
    "    ### End your code\n",
    "    \n",
    "def __next__(self):\n",
    "    ### Begin your code\n",
    "    if self.current_position>= len(self.terms):##判断位置范围\n",
    "        raise StopIteration \n",
    "    ##termID--docsID\n",
    "    termID = self.terms[self.current_position] \n",
    "    start_position, n_postings, length_in_bytes = self.postings_dict[termID]\n",
    "    self.index_file.seek(start_position)\n",
    "    postings_list = self.postings_encoding.decode(self.index_file.read(length_in_bytes))\n",
    "    self.current_position += 1\n",
    "    return termID, postings_list\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现读取之后，接下来就要实现合并。根据已知的`heaqp.merge()`函数和`lambda()`函数实现`merge()`函数，完成倒排表的合并以及磁盘写入，主要思路如下（类似于归并排序）：\n",
    "\n",
    "1. 使用 `heapq.merge()` 合并使用`lamda()`按termID排序后的索引。\n",
    "2. 遍历合并后的索引列表，将新的 `termID-curr_postings` 元组添加到merged_index中，并更新 `last_term` 与`last_posting`，将已存在的`termID`对应的文档编号列表合并到 `last_posting` 列表中以确保每个termID被添加一次。\n",
    "3. 将最后一个词添加到merged_index中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(self, indices, merged_index): #不能用查表方式\n",
    "    ### Begin your code\n",
    "    last_term = None\n",
    "    last_posting = []\n",
    "    for termID, curr_postings in heapq.merge(*indices, key=lambda x: self.term_id_map[x[0]]):\n",
    "        if termID != last_term: ## 不同则append新的term\n",
    "            if last_term:\n",
    "                merged_index.append(last_term, list(set(last_posting)))\n",
    "            last_term = termID\n",
    "            last_posting = curr_postings\n",
    "        else:## 相同\n",
    "            last_posting += curr_postings \n",
    "    if last_term:\n",
    "        merged_index.append(last_term,list(set(last_posting)))\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 布尔联合检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先实现`InvertedIndex`的子类`InvertedIndexMapper`，找到对应terms在索引文件中位置并取出它的倒排记录表。其中`_get_postings_list()`实现思路如下：\n",
    "\n",
    "根据三元组信息，打开对应文件并找到termID对应的倒排记录表的字节流，对其进行解码得到倒排记录表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_postings_list(self, term):\n",
    "    ### Begin your code\n",
    "    start_position, n_postings, length_in_bytes = self.postings_dict[term]\n",
    "    with open(self.index_file_path, 'rb+') as index_file:\n",
    "        index_file.seek(start_position)  ##起始位置\n",
    "        postings_list_bytes = index_file.read(length_in_bytes)  ##读取postings列表\n",
    "        postings_list = self.postings_encoding.decode(postings_list_bytes) ##解码\n",
    "    return postings_list\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到倒排记录表之后，实现`sorted_intersect()`函数，遍历两个有序列表并在线性时间内合并以求交集。主要思路与merge()函数和归并排序类似便不做介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_intersect(list1, list2):\n",
    "    ### Begin your code\n",
    "    i, j = 0, 0\n",
    "    intersection = []  #结果\n",
    "    while i < len(list1) and j < len(list2):\n",
    "        if list1[i] == list2[j]:\n",
    "            intersection.append(list1[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif list1[i] < list2[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return intersection\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用`sorted_intersect` 和 `InvertedIndexMapper`来实现`retrieve`函数，对于给定的包含由空格分隔tokens的字符串查询，返回包含查询中所有tokens的文档列表。主要思路如下：\n",
    "\n",
    "1. 将给定的字符串`query`进行分词处理，得到tokens并使用`term_id_map`和`lambda()`得到对应的排序好的termID存于`terms`中\n",
    "2. 使用`InvertedIndexMapper`类创建映射器对象mapper访问倒排索引获取termID对应的文档列表，使用 `sorted_intersect()` 函数依次计算两个列表的交集并保持列表排序\n",
    "3. 使用 `map()`将文档ID映射回文档名称，使用`sorted()`确保结果列表是排序的并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(self, query):\n",
    "    if len(self.term_id_map) == 0 or len(self.doc_id_map) == 0:\n",
    "        self.load()\n",
    "    ### Begin your code     \n",
    "    tokens=query.strip().split()\n",
    "    terms = list(map(lambda x : self.term_id_map[x],tokens))\n",
    "    if len(tokens) ==0:\n",
    "        return []\n",
    "    result=[]\n",
    "    with InvertedIndexMapper(self.index_name, directory=self.output_dir, postings_encoding=self.postings_encoding) as mapper:\n",
    "        result.extend(mapper[terms[0]])\n",
    "        for i in range(1,len(terms)):\n",
    "            result = sorted_intersect(result,mapper[terms[i]])\n",
    "    return sorted(list(map(lambda x : self.doc_id_map[x],result)))\n",
    "    ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过[Chapter 5](https://nlp.stanford.edu/IR-book/pdf/05comp.pdf)以及网站我们了解到了gap-encoding和可变长字节编码，参考以实现CompressedPostings类。\n",
    "\n",
    "其中gap-encoding是一种压缩技术，docID列表中，如果连续的文档ID之间的gap较小，那么这些差异可以用更少的位数来表示。\n",
    "\n",
    "可变长字节编码的基本原理是将每个整数分成多个部分，每个部分称为一个“段”（chunk），每个段包含7位有效数据（因为一个字节有8位，其中1位用于表示后续是否还有字节）。\n",
    "\n",
    "编码过程如下（解码逆过来就可以）：\n",
    "- 分割整数：将整数分成多个7位的段\n",
    "- 编码每个段：每个段的最高位（第8位）用于表示是否有后续的段，1表示还有，0表示结束\n",
    "- 拼接段：将所有段拼接形成最终的编码\n",
    "\n",
    "实现思路如下：\n",
    "\n",
    "1. `encode()`函数：将列表中第一个docID编码为字节，然后遍历列表计算docID与前一个docID之间的差值（gap），并使用`_varint_encode()`函数将这些差值编码为可变长字节。\n",
    "\n",
    "2. `decode()`函数：创建空列表存储解码后的docID，遍历字节序列使用`_varint_decode()`函数解码并将解码后的第一个整数添加到postings_list中，对于后续的整数将其与postings_list中的最后一个文档ID相加，得到实际的docID并添加到列表中。\n",
    "\n",
    "3. `_varint_encode()`函数：不断地将整数与0x7F进行与操作来获取低7位，然后将结果左移7位，直到整数为0。如果整数不为0，则将最高位设置为1，表示后续还有字节。\n",
    "\n",
    "4. `_varint_decode()`函数：不断地读取字节，并根据最高位是否为1来决定是否继续读取下一个字节，直到遇到最高位为0的字节为止。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedPostings:\n",
    "    @staticmethod\n",
    "    def _varint_encode(n): ##可变长字节编码\n",
    "        encoded = bytearray()\n",
    "        while True:\n",
    "            seven_bits = n & 0x7F\n",
    "            n >>= 7\n",
    "            if n:\n",
    "                encoded.append(seven_bits | 0x80)\n",
    "            else:\n",
    "                encoded.append(seven_bits)\n",
    "                break\n",
    "        return bytes(encoded)\n",
    "    @staticmethod\n",
    "    def _varint_decode(data, start_index): ##可变长字节解码\n",
    "        result = 0\n",
    "        shift = 0\n",
    "        while True:\n",
    "            b = data[start_index]\n",
    "            start_index += 1\n",
    "            result |= (b & 0x7F) << shift\n",
    "            if not (b & 0x80):\n",
    "                break\n",
    "            shift += 7\n",
    "        return result, start_index\n",
    "    ### End your code\n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        ### Begin your code\n",
    "        if not postings_list:\n",
    "            return b''\n",
    "        encoded = bytearray()\n",
    "        last_doc_id = postings_list[0]\n",
    "        \n",
    "        for doc_id in postings_list[1:]:\n",
    "            gap = doc_id - last_doc_id\n",
    "            encoded += CompressedPostings._varint_encode(gap)\n",
    "            last_doc_id = doc_id\n",
    "        encoded = CompressedPostings._varint_encode(postings_list[0]) + encoded\n",
    "        return bytes(encoded)\n",
    "        ### End your code\n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        ### Begin your code\n",
    "        postings_list = []\n",
    "        index = 0\n",
    "        # Decode the first docId\n",
    "        doc_id, index = CompressedPostings._varint_decode(encoded_postings_list, index)\n",
    "        postings_list.append(doc_id)\n",
    "        \n",
    "        while index < len(encoded_postings_list):\n",
    "            gap, index = CompressedPostings._varint_decode(encoded_postings_list, index)\n",
    "            doc_id += gap\n",
    "            postings_list.append(doc_id)\n",
    "        return postings_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 额外的编码方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过补充`ECCompressedPostings`的`encode` 和 `decode`方法来实现一种额外的索引压缩方式**gamma-encoding**。\n",
    "\n",
    "gamma编码的步骤如下(即`gamma_encode()`函数)：\n",
    "1. 对于数字x分解成 x=2N + M\n",
    "2. 对于N+1使用一元编码\n",
    "3. 对于M使用比特宽度为N的二进制编码\n",
    "\n",
    "其中一元编码是指：对于数字N，使用N-1个二进制1和末尾一个0表示，如数字3的一元编码为：110\n",
    "\n",
    "解码(即`gamma_decode()`函数)就是将解码反过来，从编码的字节串读取信息，去相应数量的位重建原始的数值。\n",
    "\n",
    "`encode()`函数: 遍历posting_list，对每个数字进行编码并将结果连接起来。\n",
    "`decode()`函数: 逐个解码同时更新索引以跳过已解码的部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "class ECCompressedPostings:\n",
    "    #If you need any extra helper methods you can add them here \n",
    "    ### Begin your code\n",
    "    @staticmethod\n",
    "    def gamma_encode(num):\n",
    "        if num == 0:\n",
    "            return b'\\x00'\n",
    "        N = num.bit_length() - 1 \n",
    "        M = num - (1 << N)\n",
    "        unary_encoded = b'\\x01' * N + b'\\x00'\n",
    "        num_bytes = (N + 7) // 8\n",
    "        M_encoded = M.to_bytes(num_bytes, byteorder='big')\n",
    "        if len(M_encoded) < num_bytes:\n",
    "            M_encoded = b'\\x00' * (num_bytes - len(M_encoded)) + M_encoded\n",
    "        return unary_encoded + M_encoded[-num_bytes:]\n",
    "    @staticmethod\n",
    "    def gamma_decode(encoded_str):\n",
    "        index = 0\n",
    "        if encoded_str[index] == 0x00:\n",
    "            return 0, 1\n",
    "        while index < len(encoded_str) and encoded_str[index] == 0x01:\n",
    "            index += 1\n",
    "        N = index\n",
    "        byte_length = (N + 7) // 8\n",
    "        start_index = index + 1\n",
    "        end_index = start_index + byte_length\n",
    "        M_bytes = encoded_str[start_index:end_index]\n",
    "        M = int.from_bytes(M_bytes, byteorder='big') & ((1 << N) - 1)\n",
    "        return (1 << N) + M, end_index\n",
    "\n",
    "    ### End your code\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        ### Begin your code\n",
    "        if not postings_list:  # 特殊处理空列表\n",
    "            return b''\n",
    "        encoded = bytes()\n",
    "        for num in postings_list:\n",
    "            encoded += ECCompressedPostings.gamma_encode(num)\n",
    "        return encoded\n",
    "        ### End your code\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        ### Begin your code\n",
    "        decoded_list = []\n",
    "        index = 0\n",
    "        while index < len(encoded_postings_list):\n",
    "            num, length = ECCompressedPostings.gamma_decode(encoded_postings_list[index:])\n",
    "            decoded_list.append(num)\n",
    "            index += length\n",
    "        return decoded_list\n",
    "        ### End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过此次实验，更好的理解掌握了索引的构建与压缩以及布尔检索。在实验的过程中也遇到了挺多问题，比如变量太多时不时忘记,一些库函数的使用（像extend和sorted_intersect之类的）,分词时方法的使用（尝试了不同的分词方式）以及额外编码方式中gamma编码(因为本人在网上搜到不同的两个版本所以挺模糊的，最后查看ppt又结合网络成功理解，参考了（https://tonymazn.wordpress.com/2018/09/03/%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95%E4%B9%8Belias-gamma-coding-elias-delta-coding/）其中还有delta编码但本次只实现了gamma编码)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
